{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2gelU0RVdJ1k"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "from random import randint\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageFont, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VK76Pa6TomeF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define paths\n",
    "# path = \"/content/\"\n",
    "pictures_path = os.path.join(\"/home/fizzer/ENPH353_Competition/src/pink_chicken/reading_CNN/pictures\")\n",
    "os.makedirs(pictures_path, exist_ok=True)\n",
    "\n",
    "ALPHABET_PLATES = 5000\n",
    "NUMBER_PLATES = 5000\n",
    "# Initialize the data generator with augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=8,            # Random rotation within 8 degrees\n",
    "    zoom_range=0.05,             # Small random zoom (5%)\n",
    "    brightness_range=[0.8, 2.0], # Adjust brightness between 30% and 200%\n",
    "    shear_range=2,               # Shear by 2 degrees\n",
    "    width_shift_range=0.05,      # Shift width by 5%\n",
    "    height_shift_range=0.05      # Shift height by 5%\n",
    ")\n",
    "\n",
    "def blue_mask(image):\n",
    "    # Convert the image from BGR to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the range for blue in HSV\n",
    "    lower_blue = np.array([100, 120, 70])  # Lower range for blue\n",
    "    upper_blue = np.array([140, 255, 255])  # Upper range for blue\n",
    "\n",
    "    # Create a binary mask where blue pixels are 1 and others are 0\n",
    "    blue_mask = cv2.inRange(hsv_image, lower_blue, upper_blue)\n",
    "\n",
    "    return blue_mask\n",
    "# Function to add Gaussian noise\n",
    "def add_gaussian_noise(image, mean=0, std=25):\n",
    "    gauss = np.random.normal(mean, std, image.shape)  # Generate Gaussian noise\n",
    "    noisy_image = image + gauss  # Add noise\n",
    "    noisy_image = np.clip(noisy_image, 0, 255).astype('uint8')  # Clip to valid range\n",
    "    return noisy_image\n",
    "\n",
    "# Function to add Salt-and-Pepper noise\n",
    "def add_salt_and_pepper_noise(image, salt_prob=0.02, pepper_prob=0.02):\n",
    "    noisy_image = image.copy()\n",
    "    total_pixels = image.size\n",
    "\n",
    "    # Add salt (white pixels)\n",
    "    num_salt = int(salt_prob * total_pixels)\n",
    "    coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape[:2]]\n",
    "    noisy_image[coords[0], coords[1]] = 255\n",
    "\n",
    "    # Add pepper (black pixels)\n",
    "    num_pepper = int(pepper_prob * total_pixels)\n",
    "    coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape[:2]]\n",
    "    noisy_image[coords[0], coords[1]] = 0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "# Function to add Gaussian blur\n",
    "def add_gaussian_blur(image, kernel_size=(10, 50)):\n",
    "    \"\"\"\n",
    "    Applies Gaussian blur to the image.\n",
    "    Args:\n",
    "        image: Input image (NumPy array).\n",
    "        kernel_size: Tuple representing the size of the Gaussian kernel.\n",
    "    Returns:\n",
    "        Blurred image (NumPy array).\n",
    "    \"\"\"\n",
    "    blurred_image = cv2.GaussianBlur(image, kernel_size, 0)\n",
    "    return blurred_image\n",
    "\n",
    "# Generate NUMBER_OF_PLATES with random values\n",
    "for i in range(0, ALPHABET_PLATES):\n",
    "    # Pick two random letters\n",
    "    plate_alpha = \"\"\n",
    "    for _ in range(0, 2):\n",
    "        plate_alpha += random.choice(string.ascii_uppercase)\n",
    "    \n",
    "    # Pick the last letter\n",
    "    plate_filler = random.choice(string.ascii_uppercase)\n",
    "\n",
    "    # Pick two more random letters\n",
    "    plate_num = \"\"\n",
    "    for _ in range(0, 2):\n",
    "        plate_num += random.choice(string.ascii_uppercase)\n",
    "\n",
    "    # Write plate to image\n",
    "    blank_plate = cv2.imread('/home/fizzer/ENPH353_Competition/src/pink_chicken/reading_CNN/blank_plate.png')\n",
    "\n",
    "    # Convert into a PIL image (this is so we can use the monospaced fonts)\n",
    "    blank_plate_pil = Image.fromarray(blank_plate)\n",
    "\n",
    "    # Get a drawing context\n",
    "    draw = ImageDraw.Draw(blank_plate_pil)\n",
    "    monospace = ImageFont.truetype(\n",
    "        font=\"/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf\",\n",
    "        size=165\n",
    "    )\n",
    "    draw.text(\n",
    "        xy=(48, 75),\n",
    "        text=plate_alpha +  plate_num + plate_filler,\n",
    "        fill=(255, 0, 0),  # Red text color\n",
    "        font=monospace\n",
    "    )\n",
    "\n",
    "    # Convert back to OpenCV image\n",
    "    blank_plate = np.array(blank_plate_pil)\n",
    "\n",
    "    # Expand dimensions to match the generator's input format (batch size, height, width, channels)\n",
    "    blank_plate = np.expand_dims(blank_plate, axis=0)\n",
    "\n",
    "    # Apply augmentations using the generator\n",
    "    augmented_iter = datagen.flow(blank_plate, batch_size=1)\n",
    "\n",
    "    # Get the augmented image\n",
    "    value = next(augmented_iter)  # Get the first (and only) batch\n",
    "    augmented_image = value[0].astype('uint8')  # Convert back to uint8\n",
    "\n",
    "    augmented_image = add_gaussian_noise(augmented_image)\n",
    "\n",
    "    augmented_image = add_salt_and_pepper_noise(augmented_image)\n",
    "\n",
    "    augmented_image = add_gaussian_blur(augmented_image, kernel_size=(5, 5))\n",
    "\n",
    "    augmented_image = blue_mask(augmented_image)\n",
    "\n",
    "\n",
    "    # Save the augmented plate image with noise or blur\n",
    "    cv2.imwrite(os.path.join(\n",
    "        pictures_path,\n",
    "        f\"plate_{plate_alpha}{plate_num[1]}{plate_filler}.png\"\n",
    "    ), augmented_image)\n",
    "\n",
    "\n",
    "# Generate NUMBER_OF_PLATES with random values\n",
    "for i in range(0, NUMBER_PLATES):\n",
    "    # Pick two random letters\n",
    "    plate_alpha = \"\"\n",
    "    for _ in range(0, 2):\n",
    "        plate_alpha += random.choice(string.ascii_uppercase)\n",
    "\n",
    "\n",
    "    # Pick two more random numbers\n",
    "    num = random.randint(0,9)\n",
    "    plate_num = \"{:02d}\".format(num)\n",
    "\n",
    "    # Write plate to image\n",
    "    blank_plate = cv2.imread('/home/fizzer/ENPH353_Competition/src/pink_chicken/reading_CNN/blank_plate.png')\n",
    "\n",
    "    # Convert into a PIL image (this is so we can use the monospaced fonts)\n",
    "    blank_plate_pil = Image.fromarray(blank_plate)\n",
    "\n",
    "    # Get a drawing context\n",
    "    draw = ImageDraw.Draw(blank_plate_pil)\n",
    "    monospace = ImageFont.truetype(\n",
    "        font=\"/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf\",\n",
    "        size=165\n",
    "    )\n",
    "    draw.text(\n",
    "        xy=(48, 75),\n",
    "        text=plate_alpha + \" \"+ plate_num,\n",
    "        fill=(255, 0, 0),  # Red text color\n",
    "        font=monospace\n",
    "    )\n",
    "\n",
    "    # Convert back to OpenCV image\n",
    "    blank_plate = np.array(blank_plate_pil)\n",
    "\n",
    "    # Expand dimensions to match the generator's input format (batch size, height, width, channels)\n",
    "    blank_plate = np.expand_dims(blank_plate, axis=0)\n",
    "\n",
    "    # Apply augmentations using the generator\n",
    "    augmented_iter = datagen.flow(blank_plate, batch_size=1)\n",
    "\n",
    "    # Get the augmented image\n",
    "    value = next(augmented_iter)  # Get the first (and only) batch\n",
    "    augmented_image = value[0].astype('uint8')  # Convert back to uint8\n",
    "\n",
    "    augmented_image = add_gaussian_noise(augmented_image)\n",
    "\n",
    "    augmented_image = add_salt_and_pepper_noise(augmented_image)\n",
    "\n",
    "    augmented_image = add_gaussian_blur(augmented_image, kernel_size=(5, 5))\n",
    "\n",
    "    augmented_image = blue_mask(augmented_image)\n",
    "\n",
    "\n",
    "    # Save the augmented plate image with noise or blur\n",
    "    cv2.imwrite(os.path.join(\n",
    "        pictures_path,\n",
    "        f\"plate_{plate_alpha}{plate_num}.png\"\n",
    "    ), augmented_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3JG2UDDVXhh",
    "outputId": "55c8d300-3672-4d0f-b6aa-924c81a6bd39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the directory: 8517\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory you want to count the files in\n",
    "directory = '/home/fizzer/ENPH353_Competition/src/pink_chicken/reading_CNN/pictures'\n",
    "\n",
    "# List all files and directories in the specified directory\n",
    "all_files = os.listdir(directory)\n",
    "\n",
    "# Filter only files (ignoring directories)\n",
    "file_count = len([f for f in all_files if os.path.isfile(os.path.join(directory, f))])\n",
    "\n",
    "print(f\"Number of files in the directory: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "W-GwG17dd-uI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpJ7iXJX58to"
   },
   "source": [
    "We create functions for One-hot Encoding, Cropping license Plate, and Processing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rWqRuqP554E6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define character set for one-hot encoding (0-9 and A-Z)\n",
    "characters = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "# Match the character to its location in string characters\n",
    "char_to_onehot = {char: i for i, char in enumerate(characters)}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "One-hot Encoding\n",
    "- We create a 36 letter vector\n",
    "- Match each character to a vector\n",
    "\"\"\"\n",
    "# Encode a specific character to a vector\n",
    "def one_hot_encode(char):\n",
    "    one_hot = np.zeros(36)  # 36-dimensional vector\n",
    "    one_hot[char_to_onehot[char]] = 1  # Set the correct index to 1\n",
    "    return one_hot\n",
    "\n",
    "\"\"\"\n",
    "Crop License Plate\n",
    "- Use trial-and-error to find the correct bounds for each character\n",
    "- Crop each rectangle and\n",
    "\"\"\"\n",
    "# Crop the license plate by each letter into 4 equal images\n",
    "def crop_license_plate(image, char_width=100, char_height=150, left_start=(45, 85), middle_width=97):\n",
    "\n",
    "    # Define the corners for each character region\n",
    "    char1 = (left_start, (left_start[0] + char_width, left_start[1] + char_height))\n",
    "    char2 = ((char1[1][0], left_start[1]), (char1[1][0] + char_width, left_start[1] + char_height))\n",
    "    char3 = ((char2[1][0] + middle_width, left_start[1]), (char2[1][0] + char_width + middle_width, left_start[1] + char_height))\n",
    "    char4 = ((char3[1][0], left_start[1]), (char3[1][0] + char_width, left_start[1] + char_height))\n",
    "\n",
    "    # Draw 4 rectangles\n",
    "    rectangles = [char1, char2, char3, char4]\n",
    "\n",
    "    # Initiate array to store the cropped images\n",
    "    cropped_images = []\n",
    "\n",
    "    # Crop the 4 rectangles\n",
    "    for top_left, bottom_right in rectangles:\n",
    "        cropped_image = image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
    "        cropped_image_resized = cv2.resize(cropped_image, (char_width, char_height))\n",
    "        # Save into array\n",
    "        cropped_images.append(cropped_image_resized)\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "\"\"\"\n",
    "Process the Images\n",
    "- We iterate through each image in the directory\n",
    "- Crop each image into 4 rectangles, one character per image\n",
    "- Match the name of the license plate to each cropped character\n",
    "- Create training data ourselves\n",
    "\"\"\"\n",
    "directory = '/home/fizzer/ENPH353_Competition/src/pink_chicken/reading_CNN/pictures'\n",
    "def process_license_plate_files(directory):\n",
    "    X_data = []  # Will store all cropped images (input for CNN)\n",
    "    Y_data = []  # Will store all one-hot encoded labels\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.png'):  # Only process PNG files\n",
    "            # Get the path to the image\n",
    "            image_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Extract the file name by splitting it at '_' and '.'\n",
    "            label = filename.split('_')[1].split('.')[0]  # \"EP68\"\n",
    "\n",
    "            # Read the image\n",
    "            plate_image = cv2.imread(image_path)\n",
    "            # grayscale_image = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)\n",
    "            # _, binary_image = cv2.threshold(grayscale_image, 127, 255, cv2.THRESH_BINARY)\n",
    "            # inverted_image = cv2.bitwise_not(binary_image)\n",
    "            original_height, original_width = plate_image.shape[:2]\n",
    "\n",
    "\n",
    "            # Downscale the image by a factor of 2\n",
    "            downscaled_image = cv2.resize(plate_image,\n",
    "                                          (original_width // 4, original_height // 4),\n",
    "                                          interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Upscale the image back to its original size\n",
    "            plate_image = cv2.resize(downscaled_image,\n",
    "                                        (original_width, original_height),\n",
    "                                        interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Crop the license plate into subsections by calling previous function\n",
    "            cropped_images = crop_license_plate(plate_image)\n",
    "\n",
    "            kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "            cropped_images = [cv2.erode(image, kernel, iterations=3) for image in cropped_images]\n",
    "\n",
    "            # One-hot encode the label and associate with each cropped image\n",
    "            for i, char in enumerate(label):\n",
    "                one_hot_label = one_hot_encode(char)  # One-hot encode each character\n",
    "                X_data.append(cropped_images[i])      # Add cropped image to X_data\n",
    "                Y_data.append(one_hot_label)          # Add one-hot label to Y_data\n",
    "\n",
    "    # Convert lists to NumPy arrays for easier manipulation\n",
    "    X_data = np.array(X_data)\n",
    "    Y_data = np.array(Y_data)\n",
    "\n",
    "    return np.array(X_data), np.array(Y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoXtHfGvJg9i"
   },
   "source": [
    "We call the process license plate files function and see if we are correctly producing and matching the character to the one-hot encoder. The shape of X and Y can tell us how large our data library is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "id": "HZ-0lKFIHSav",
    "outputId": "64eec925-6891-4552-fcfe-6bb7c089b4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (34068, 150, 100, 3)\n",
      "Y shape: (34068, 36)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = process_license_plate_files('/home/fizzer/ENPH353_Competition/src/pink_chicken/reading_CNN/pictures')\n",
    "\n",
    "# Print the shapes of X and Y\n",
    "print(f\"X shape: {X_data.shape}\")  # Number of cropped images and their dimensions\n",
    "print(f\"Y shape: {Y_data.shape}\")  # Number of one-hot encoded labels and their size\n",
    "\n",
    "# for i in range(5):  # Display the first 5 images to check if it is good\n",
    "#     cv2.imshow('i', X_data[i])\n",
    "#     cv2.waitKey(3)\n",
    "#     print(Y_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9FzFMw_sI1cm",
    "outputId": "4791a2e9-3976-41a7-eefc-ad4cf0b9908d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34068, 150, 100, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 17:32:41.506131: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5825520000 exceeds 10% of free system memory.\n",
      "2024-12-03 17:32:56.061247: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.42GiB (rounded to 5825520128)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-12-03 17:32:56.061285: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2024-12-03 17:32:56.061302: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 17, Chunks in use: 17. 4.2KiB allocated for chunks. 4.2KiB in use in bin. 144B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061313: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061324: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 2, Chunks in use: 2. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 2.7KiB client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061334: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 1, Chunks in use: 0. 3.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061344: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061353: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061363: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061372: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061382: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061392: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061401: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061410: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061420: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061429: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061440: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 4.44MiB allocated for chunks. 4.44MiB in use in bin. 4.44MiB client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061449: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061459: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061468: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061481: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061490: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061500: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 1. 5.11GiB allocated for chunks. 1.36GiB in use in bin. 1.36GiB client-requested in use in bin.\n",
      "2024-12-03 17:32:56.061512: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 5.42GiB was 256.00MiB, Chunk State: \n",
      "2024-12-03 17:32:56.061525: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 3.75GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 1.8KiB | Requested Size: 1.7KiB | in_use: 1 | bin_num: -1\n",
      "2024-12-03 17:32:56.061534: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 5492572160\n",
      "2024-12-03 17:32:56.061544: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000000 of size 256 next 1\n",
      "2024-12-03 17:32:56.061554: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000100 of size 1280 next 2\n",
      "2024-12-03 17:32:56.061563: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000600 of size 256 next 3\n",
      "2024-12-03 17:32:56.061571: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000700 of size 256 next 4\n",
      "2024-12-03 17:32:56.061580: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000800 of size 256 next 5\n",
      "2024-12-03 17:32:56.061588: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000900 of size 256 next 6\n",
      "2024-12-03 17:32:56.061597: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000a00 of size 256 next 7\n",
      "2024-12-03 17:32:56.061605: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000b00 of size 256 next 8\n",
      "2024-12-03 17:32:56.061614: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000c00 of size 256 next 9\n",
      "2024-12-03 17:32:56.061623: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9ba8000d00 of size 1456380160 next 10\n",
      "2024-12-03 17:32:56.061632: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bfecea600 of size 4660480 next 11\n",
      "2024-12-03 17:32:56.061641: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15c300 of size 256 next 12\n",
      "2024-12-03 17:32:56.061650: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15c400 of size 256 next 13\n",
      "2024-12-03 17:32:56.061658: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15c5"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_data\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Verify shape\u001b[39;00m\n\u001b[1;32m     71\u001b[0m Y_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Y_data)\n\u001b[0;32m---> 72\u001b[0m history_conv \u001b[38;5;241m=\u001b[39m \u001b[43mconv_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m reset_weights_tf2(conv_model)\n\u001b[1;32m     82\u001b[0m conv_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00 of size 256 next 14\n",
      "2024-12-03 17:32:56.061667: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15c600 of size 256 next 15\n",
      "2024-12-03 17:32:56.061675: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15c700 of size 256 next 17\n",
      "2024-12-03 17:32:56.061684: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15c800 of size 256 next 18\n",
      "2024-12-03 17:32:56.061692: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15c900 of size 256 next 16\n",
      "2024-12-03 17:32:56.061701: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15ca00 of size 256 next 19\n",
      "2024-12-03 17:32:56.061709: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15cb00 of size 256 next 22\n",
      "2024-12-03 17:32:56.061718: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f9bff15cc00 of size 3072 next 20\n",
      "2024-12-03 17:32:56.061727: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f9bff15d800 of size 1792 next 21\n",
      "2024-12-03 17:32:56.061736: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f9bff15df00 of size 4031521024 next 18446744073709551615\n",
      "2024-12-03 17:32:56.061744: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2024-12-03 17:32:56.061756: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 17 Chunks of size 256 totalling 4.2KiB\n",
      "2024-12-03 17:32:56.061765: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-12-03 17:32:56.061775: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1792 totalling 1.8KiB\n",
      "2024-12-03 17:32:56.061784: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4660480 totalling 4.44MiB\n",
      "2024-12-03 17:32:56.061793: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1456380160 totalling 1.36GiB\n",
      "2024-12-03 17:32:56.061802: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 1.36GiB\n",
      "2024-12-03 17:32:56.061812: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 5492572160 memory_limit_: 5492572160 available bytes: 0 curr_region_allocation_bytes_: 10985144320\n",
      "2024-12-03 17:32:56.061824: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      5492572160\n",
      "InUse:                      1461048064\n",
      "MaxInUse:                   1461050880\n",
      "NumAllocs:                          26\n",
      "MaxAllocSize:               1456380160\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-12-03 17:32:56.061835: W tensorflow/tsl/framework/bfc_allocator.cc:497] ***************************_________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/63435679\n",
    "def reset_weights_tf2(model):\n",
    "\n",
    "  # This loop iterates through each layer\n",
    "  for ix, layer in enumerate(model.layers):\n",
    "      # Check that there is 'kernel_initializer', 'bias_initializer'\n",
    "      if (hasattr(model.layers[ix], 'kernel_initializer') and\n",
    "          hasattr(model.layers[ix], 'bias_initializer')):\n",
    "          # Finds weight & bias initializer\n",
    "          weight_initializer = model.layers[ix].kernel_initializer\n",
    "          bias_initializer = model.layers[ix].bias_initializer\n",
    "\n",
    "          # Finds the current weights and biases\n",
    "          old_weights, old_biases = model.layers[ix].get_weights()\n",
    "\n",
    "          # Reinitializes weight and bias\n",
    "          model.layers[ix].set_weights([\n",
    "              weight_initializer(shape=old_weights.shape),\n",
    "              bias_initializer(shape=len(old_biases))])\n",
    "\n",
    "\n",
    "conv_model = models.Sequential()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=5,\n",
    "                               restore_best_weights=True)\n",
    "# Second Convolutional Block\n",
    "conv_model.add(layers.Conv2D(16, (3, 3), activation='relu'))  # Reduce filters\n",
    "conv_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Third Convolutional Block\n",
    "conv_model.add(layers.Conv2D(32, (3, 3), activation='relu'))  # Reduce filters\n",
    "conv_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Fourth Convolutional Block\n",
    "conv_model.add(layers.Conv2D(64, (3, 3), activation='relu'))  # Reduce filters\n",
    "conv_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten and Dense Layers\n",
    "conv_model.add(layers.Flatten())\n",
    "conv_model.add(layers.Dropout(0.3))  # Lower dropout to preserve more information\n",
    "conv_model.add(layers.Dense(512, activation='relu'))  # Reduce neurons\n",
    "conv_model.add(layers.Dropout(0.5))\n",
    "conv_model.add(layers.Dense(128, activation='relu'))  # Reduce neurons\n",
    "conv_model.add(layers.Dropout(0.5))\n",
    "conv_model.add(layers.Dense(36, activation='softmax'))  # Output layer for 36 classes\n",
    "\n",
    "\n",
    "# Validation split: 10% of data will be used for validation\n",
    "VALIDATION_SPLIT = 0.05\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "conv_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
    "                   metrics=['acc'])\n",
    "\n",
    "\n",
    "# Batch size: How many samples will be processed in one step\n",
    "# Samples: From your training data set\n",
    "# Step size: Samples / Batch size\n",
    "\n",
    "X_data = np.array(X_data).astype('float32') / 255.0\n",
    "print(X_data.shape)  # Verify shape\n",
    "\n",
    "Y_data = np.array(Y_data)\n",
    "history_conv = conv_model.fit(X_data, Y_data,\n",
    "                              validation_split=VALIDATION_SPLIT,\n",
    "                              epochs=1,\n",
    "                              batch_size= 32,\n",
    "                              callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reset_weights_tf2(conv_model)\n",
    "conv_model.summary()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history_conv.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history_conv.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history_conv.history['loss'], label='Training Loss')\n",
    "plt.plot(history_conv.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate predictions\n",
    "Y_pred = conv_model.predict(X_data)\n",
    "\n",
    "# Convert predictions and true labels to class indices\n",
    "Y_pred_labels = np.argmax(Y_pred, axis=1) # My predicted data\n",
    "Y_true_labels = np.argmax(Y_data, axis=1) # Actual data we are matching\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(Y_true_labels, Y_pred_labels)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=characters, yticklabels=characters)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ot5PDwqImiL4",
    "outputId": "1959a5a4-7c7e-4877-8333-c0cef9a46ad3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fizzer/.local/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "conv_model.save('GOOD_MODEL.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
